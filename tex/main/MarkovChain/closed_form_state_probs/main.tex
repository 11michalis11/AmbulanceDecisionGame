\subsection{Closed-form formula for state probabilities}

This section aims to describe a closed form formula that gets the state probabilities array \(\pi\) for a given Markov chain model.

\subsubsection{Parameters}
The inputs of the formula are the number of servers \(C\), the threshold \(T\), the system capacity \(N\) and the parking capacity \(M\). Additional parameters of the model are the ambulance arrival rate, the others' arrival rate and the service rate, but for the purpose of this section these will remain unknown (\(\lambda^A, \lambda^o, \mu\)). More specifically, the way these parameters are translated into the model are:

\begin{itemize}
    \item \textbf{Number of servers (\(C\)):} All service rates \(\mu\) in the Markov chain are multiplied by a coefficient equal to \(v\) for a state \((u,v)\) that stops increasing at \(v=C\). Thus, the coefficients of the service rate have a lower bound of \(0\) and an upper bound of \(C\).
    \item \textbf{Threshold (\(T\)):} Determines the length of the left \textit{arm} of the model. In essence the threshold acts as a breakpoint between states where \(u=0\) and states where \(0 \leq u \leq M\). Increasing \(T\) results in having more set of states where \(u\) can only be \(0\).
    \item \textbf{System capacity (\(N\)):} Is the upper bound of \(v\) for all states \((u,v)\).
    \item \textbf{Parking capacity (\(M\)):} Is the upper bound of \(u\) for all states \((u,v)\) such that \(u \geq T\).
\end{itemize}


\subsubsection{Example figure of Markov Model}

\begin{figure}[h]
    \centering
    \input{MarkovChain/closed_form_state_probs/example_model_1352/main.tex}
    \caption{\(C=1, T=3, N=5, M=2\)}
    \label{fig:Markov_1352_example_for_closed_form}
\end{figure}

In figure \ref{fig:Markov_1352_example_for_closed_form} an example of such Markov model is shown where \(C=1\) meaning the only coefficient in front of any \(\mu\) is going to be \(1\), \(T=3\) which means that the \textit{left arm} of the model has a length of \(3\), \(N=5\) that indicates that the right-most states \((u,v)\) are of the form \((u,5)\) and \(M=2\) that equivalently shows that the bottom states are of the form \((2,v)\).

\subsubsection{Graph theoretical approach for state probabilities}

An additional approach that one may consider to get the state probabilities is the graph theoretical approach for state probabilities. Thus, it can be assumed that a Markov chain model \(M\) can be translated as a weighted directed graph \(G_M\) where every edge has a weight that corresponds to the rate of the edges of the Markov chain. 

A \textit{directed spanning tree} of a directed graph is defined as a subset of the graph that visits all the vertices of the graph and does not include any cycles. Unlike undirected spanning trees, directed ones also have a root which means that a directed spanning tree that is rooted at a vertex \(v\) has to have a path from any other vertex to vertex \(v\). For example, consider the graph shown in figure \ref{fig:example_spanning_tree}. The graph points out a spanning tree that is rooted at vertex 3.


\begin{figure}[h]
    \centering
    \begin{tikzpicture}
        \node[state](u1){1};
        \node[state, right=of u1](u2){2};
        \node[state, right=of u2](u3){3};
        \node[state, right=of u3](u4){4};
        \node[state, below=of u2](u5){5};
        \node[state, below=of u3](u6){6};
        \node[state, below=of u4](u7){7};
        \draw[->, thick] (u1) -- (u2);
        \draw[->, thick] (u2) -- (u3);
        \draw[->, thick] (u4) -- (u3);
        \draw[->, thick] (u5) -- (u2);
        \draw[->, thick] (u6) -- (u5);
        \draw[->, thick] (u7) -- (u6);
    \end{tikzpicture}
    \caption{Spanning tree of a graph rooted at vertex 3}
    \label{fig:example_spanning_tree}
\end{figure}

Additionally, let us denote the set of all spanning trees of \(G\) as \(T(G)\) and the subset of \(T(G)\) that includes only the spanning trees that are rooted at vertex \(v\) as \(T_v(G)\). The weight of a spanning tree \(t\) can be defined as the product of the weights of the edges it contains: 
\[w(t)=\prod_{e \in t} w(e)\]



\textbf{Theorem: Markov chain tree theorem} \cite{markov-chain-tree-theorem} \newline
\textit{Let M be an irreducible Markov chain on n states with stationary distribution \(\pi_1, \pi_2, \dots, \pi_n\). Let \(G_M\) be the directed graph associated with \(M\). Then the probability of being at state \(u\) is given by:}

\begin{equation}\label{markov-chain-tree-theorem}
    \pi_i = \frac{\sum_{t \in T_i(G_M)} w(t)}{\sum_{t \in T(G_M)}w(t)}
\end{equation}

Equation \ref{markov-chain-tree-theorem} states that the probability of being at state \(u\) can be found by dividing the sum of the weights of all spanning trees rooted at \(u\) by the sum of the weights of all spanning trees of the graph. Let us ignore the denominator of that fraction for now and focus only on the numerator denoted as \(\tilde{\pi}_i=\sum_{t \in T_i(G_M)} w(t)\)

 

\newpage
\subsubsection{Spanning Trees rooted at \((0,0)\)}

Let us now consider some examples of spanning trees that are rooted at \((0,0)\). For each of the following examples the complete model is shown, then all possible spanning trees rooted at \((0,0)\) along with the weight associated with each spanning tree and finally the value of \(\tilde{\pi}_{(0,0)}\) which is the sum of all the weights of the spanning trees.

\begin{figure}[h]
    \centering
    \input{MarkovChain/closed_form_state_probs/example_model_1121/main.tex}
\end{figure}

\begin{multicols}{2}
    \begin{center}
        \input{MarkovChain/closed_form_state_probs/spanning_trees_1121/main_0.tex}
    \end{center}

    \begin{flalign*}
        \xrightarrow{\hspace*{2cm}} \hspace{1cm} \lambda^A \mu^3
    \end{flalign*}
\end{multicols}


\begin{multicols}{2}
    \begin{center}
        \input{MarkovChain/closed_form_state_probs/spanning_trees_1121/main_1.tex}
    \end{center}

    \begin{flalign*}
        \xrightarrow{\hspace*{2cm}} \hspace{1cm} \mu^4
    \end{flalign*}
\end{multicols}

\begin{equation*}
    \tilde{\pi}_{(0,0)} = \mu^4 + \lambda^A \mu^3
\end{equation*}



\newpage
\begin{figure}[h]
    \centering
    \input{MarkovChain/closed_form_state_probs/example_model_1131/main.tex}
\end{figure}


\begin{multicols}{2}
    \begin{figure}[H]
        \centering
        \scalebox{0.7}{\input{MarkovChain/closed_form_state_probs/spanning_trees_1131/main_0.tex}}
    \end{figure}

    \begin{flalign*}
        \xrightarrow{\hspace*{2cm}} \hspace{1cm} (\lambda^A)^2 \mu^4
    \end{flalign*}
\end{multicols}

\begin{multicols}{2}
    \begin{figure}[H]
        \centering
        \scalebox{0.7}{\input{MarkovChain/closed_form_state_probs/spanning_trees_1131/main_1.tex}}
    \end{figure}

    \begin{flalign*}
        \xrightarrow{\hspace*{2cm}} \hspace{1cm} \lambda^A \mu^5
    \end{flalign*}
\end{multicols}

\begin{multicols}{2}
    \begin{figure}[H]
        \centering
        \scalebox{0.7}{\input{MarkovChain/closed_form_state_probs/spanning_trees_1131/main_2.tex}}
    \end{figure}

    \begin{flalign*}
        \xrightarrow{\hspace*{2cm}} \hspace{1cm} \lambda^A \mu^5
    \end{flalign*}
\end{multicols}

\begin{multicols}{2}
    \begin{figure}[H]
        \centering
        \scalebox{0.7}{\input{MarkovChain/closed_form_state_probs/spanning_trees_1131/main_3.tex}}
    \end{figure}

    \begin{flalign*}
        \xrightarrow{\hspace*{2cm}} \hspace{1cm} \lambda^A \lambda^o \mu^4
    \end{flalign*}
\end{multicols}

\begin{multicols}{2}
    \begin{figure}[H]
        \centering
        \scalebox{0.7}{\input{MarkovChain/closed_form_state_probs/spanning_trees_1131/main_4.tex}}
    \end{figure}

    \begin{flalign*}
        \xrightarrow{\hspace*{2cm}} \hspace{1cm} \mu^6
    \end{flalign*}
\end{multicols}

\begin{equation*}
    \tilde{\pi}_{(0,0)} = (\lambda^A)^2 \mu^4 + 2 \lambda^A \mu^5 + \lambda^A \lambda^o \mu^4 + \mu^6
\end{equation*}


\newpage
\begin{figure}[h]
    \centering
    \input{MarkovChain/closed_form_state_probs/example_model_1122/main.tex}
\end{figure}

\begin{multicols}{4}
    \begin{figure}[H]
        \centering
        \scalebox{0.6}{\input{MarkovChain/closed_form_state_probs/spanning_trees_1122/main_0.tex}}
    \end{figure}
    \vspace*{\fill}
    \columnbreak
    \vspace*{0cm}
    \begin{equation*}
        (\lambda^A)^2 \mu^4
    \end{equation*}
    \vspace*{\fill}
    \columnbreak
    \begin{figure}[H]
        \centering
        \scalebox{0.6}{\input{MarkovChain/closed_form_state_probs/spanning_trees_1122/main_1.tex}}
    \end{figure}
    \vspace*{\fill}
    \columnbreak
    \vspace*{0.3cm}
    \begin{equation*}
        \lambda^A \mu^5
    \end{equation*}
\end{multicols}

\begin{multicols}{4}
    \begin{figure}[H]
        \centering
        \scalebox{0.6}{\input{MarkovChain/closed_form_state_probs/spanning_trees_1122/main_2.tex}}
    \end{figure}
    \vspace*{\fill}
    \columnbreak
    \vspace*{0.3cm}
    \begin{equation*}
        \lambda^A \mu^5
    \end{equation*}
    \vspace*{\fill}
    \columnbreak
    \begin{figure}[H]
        \centering
        \scalebox{0.6}{\input{MarkovChain/closed_form_state_probs/spanning_trees_1122/main_3.tex}}
    \end{figure}
    \vspace*{\fill}
    \columnbreak
    \vspace*{0.3cm}
    \begin{equation*}
        \mu^6
    \end{equation*}
\end{multicols}


\begin{equation*}
    \tilde{\pi}_{(0,0)} = (\lambda^A)^2 \mu^4 + 2 \lambda^A \mu^5 + \mu^6
\end{equation*}

\newpage
\subsubsection{Conjecture of adding rows}

Let us consider three Markov models with the same number of servers \(C=1\), the same threshold \(T=1\), the same system capacity \(N=2\) but different parking capacity \(M=1\), \(M=2\) and \(M=3\).


\begin{multicols}{3}
    \begin{figure}[H]
        \centering
        \scalebox{0.8}{\input{MarkovChain/closed_form_state_probs/example_model_1121/main.tex}}
        \caption{\(M=1\)}
    \end{figure}
    \columnbreak
    \begin{figure}[H]
        \centering
        \scalebox{0.8}{\input{MarkovChain/closed_form_state_probs/example_model_1122/main.tex}}
        \caption{\(M=2\)}
    \end{figure}
    \begin{figure}[H]
        \centering
        \scalebox{0.8}{\input{MarkovChain/closed_form_state_probs/example_model_1123/main.tex}}
        \caption{\(M=3\)}
    \end{figure}
\end{multicols}

By increasing the parking capacity of the model it can be easily observed that the number of spanning trees rooted at \((0,0)\) increases as well since more combinations of paths can be generated using the new edges and vertices. The corresponding values of \(\tilde{\pi}_{(0,0)}\) of the three models are:

\begin{align}
    M = 1: \tilde{\pi}_{(0,0)} &= \mu^4 + \mu^3 \lambda_A = \mu^3 (\mu + \lambda^A) \label{eq:rows-conjecture-1}\\
    M = 2: \tilde{\pi}_{(0,0)} &= \mu^6 + 2\mu^5 \lambda_A + \mu^4 (\lambda^A)^2 = \mu^4(\mu^2 + 2\mu \lambda_A + (\lambda^A)^2) = \mu^4 (\mu + \lambda^A) ^ 2 \label{eq:rows-conjecture-2}\\
    M = 3: \tilde{\pi}_{(0,0)} &= \mu^8 + 3 \mu^7 \lambda^A + 3 \mu^6 (\lambda^A)^2 + \mu^5(\lambda^A)^3 \nonumber \\
    &= \mu^5 (\mu^3 + 3 \mu ^2 \lambda^A + 3 \mu (\lambda^A)^2 + (\lambda^A)^3) \nonumber \\
    &= \mu^5 (\mu + \lambda^A) ^ 3 \label{eq:rows-conjecture-3}
\end{align}

It can be observed from equations (\ref{eq:rows-conjecture-1}),( \ref{eq:rows-conjecture-2}) and (\ref{eq:rows-conjecture-3}), that there is a noticeable relationship between them. Thus, a generalised formula for the value of \(\tilde{\pi}_{(0,0)}\) when \(C=1, T=1 \text{ and } N=1\) is given by:

\begin{equation}\label{eq:rows-conjecture-general}
    \tilde{\pi}_{(0,0)} = \mu^{(N+M)} (\mu + \lambda^A)^M
\end{equation}

It is important to note here that the above property holds when the system capacity is greater than one as well (\(N \geq 1\)). For instance let us consider a Markov model with \(C\) number of servers, a threshold of \(T\), a system capacity of \(N\) and a parking capacity of \(M\). The equivalent values of \(\tilde{\pi}_{(0,0)}\) can be expressed in terms of an unknown function \(k(C,T,N)\) as:

\begin{equation}
    \tilde{\pi}_{(0,0)} = \mu^{(N+M)} (k(C,T,N))^M
\end{equation}


\subsubsection{Matrix-tree theorem for directed graphs (Kirchhoff's theorem) \cite{matrix-tree-theorem}: }
\textit{The number of directed spanning trees rooted at a state \(i\) can be found by calculating the determinant of the Laplacian matrix \(Q\) of the directed graph and removing row \(i\) and column \(i\).}


\subsubsection{Conjecture of adding columns}
Having equation (\ref{eq:rows-conjecture-general}) it only remains to get the function \(k(C,T,N)\) that is independent of the value of the parking capacity. Thus, in this section only models with a parking capacity of \(M=1\) will be considered and will be referred to as the base cases. 

\begin{multicols}{2}
    \begin{figure}[H]
        \centering
        \scalebox{0.9}{\input{MarkovChain/closed_form_state_probs/example_model_1121/main.tex}}
    \end{figure}
    \columnbreak
    \begin{figure}[H]
        \centering
        \scalebox{0.9}{\input{MarkovChain/closed_form_state_probs/example_model_1131/main.tex}}
    \end{figure}
\end{multicols}
\vspace{-0.5cm}
\begin{alignat}{2} \label{eq:00_rate_1131}
    \hspace{4em} & \tilde{\pi}_{(0,0)} = \mu^3[\lambda^a + \mu] \hspace{8em} & \tilde{\pi}_{(0,0)} = \mu^4[(\lambda^A)^2 + \lambda^A \lambda^o + 2\lambda^A \mu + \mu^2] 
\end{alignat}


\begin{figure}[h]
    \centering
    \scalebox{0.8}{\input{MarkovChain/closed_form_state_probs/example_model_1141/main.tex}}
\end{figure}
\begin{equation}\label{eq:00_rate_1141}
    \tilde{\pi}_{(0,0)} = \mu^5[(\lambda^A)^3 + 2(\lambda^A)^2 \lambda^o + 3(\lambda^A)^2 \mu + \lambda^A (\lambda^o)^2 + 2\lambda^A \lambda^o \mu + 3\lambda^A \mu^2 + \mu^3]
\end{equation}

\begin{figure}[h]
    \centering
    \scalebox{0.8}{\input{MarkovChain/closed_form_state_probs/example_model_1151/main.tex}}
\end{figure}
\begin{align}\label{eq:00_rate_1151}
    \tilde{\pi}_{(0,0)} =& \mu^6[(\lambda^A)^4 + 3(\lambda^A)^3 \lambda^o + 4(\lambda^A)^3 \mu + 3(\lambda^A)^2 (\lambda^o)^2 + 6(\lambda^A)^2 \lambda^o \mu \\
    & + 6(\lambda^A)^2 \mu^2 + \lambda^A (\lambda^o)^3 + 2\lambda^A (\lambda^o)^2 \mu + 3\lambda^A \lambda^o \mu^2 + 4\lambda^A \mu^3 + \mu^4] \nonumber
\end{align}

\newpage
As explained in equation \ref{eq:rows-conjecture-general} the expressions defined above can boil down to a general form equation of the form \(\tilde{\pi}_{(0,0)} = \mu^{(N+M)} (k(C,T,N))^M\). The only thing missing is an expression for \(k(C,T,N)\). An initial attempt to get such an expression can be seen below:

\begin{align}\label{eq:columns-conjecture-general}
    k(C,T,N) &= \sum_{p_1=0}^{C-1} \sum_{p_2=0}^{C-p_1-1} \sum_{p_3=C - p_1 - p_2 - 1}^{C - p_1 - p_2 - 1} G(p_1, p_2, p_3) (\lambda^A)^{p_1} (\lambda^o)^{p_2} \mu^{p_3} \nonumber \\ 
    &= \sum_{p_1=0}^{C-1} \sum_{p_2=0}^{C-p_1-1} G(p_1, p_2, C-p_1-p_2-1) (\lambda^A)^{p_1} (\lambda^o)^{p_2} \mu^{C-p_1-p_2-1} 
\end{align}

In equation \ref{eq:columns-conjecture-general} the coefficient function \(G(p_1,p_2,p_3)\) is introduced were takes as arguments the powers of \(\lambda^A, \lambda^o \text{ and } \mu\). Note here that \(p_3\), the power of \(\mu\), is defined as \(p_3=C-p_1-p_2-1\) since for all base models they need to satisfy \(p_1 + p_2 + p_3 = C-1\). For the starting coefficients of the model the function \(G(p_1,p_2,p_3)\) gives the values of the coefficients and is defined as:

\begin{equation} \label{eq:coefficient-function}
    G(p_1,p_2,p_3) = 
    \begin{cases}
        0 & \text{if } p_1 = 0 \text{ and } p_2 > 0 \vspace{0.1cm} \\
        1 & \text{if } p_1, p_2 = 0 \text{ and } p_3 > 0 \vspace{0.2cm} \\
        \binom{p_1 + p_3}{p_3} & \text{if } p_2 = 0 \text{ and } p_1 > 0 \vspace{0.2cm} \\
        \binom{p_1 + p_2 - 1}{p_2} & \text{if } p_3 = 0 \text{ and } p_1, p_2 > 0 \vspace{0.2cm} \\
        p_3 + 1 & \text{if } p_1 = 1 \vspace{0.2cm} \\
        \binom{p_1 + p_3 + 1}{p_1} + p_3 \binom{p_1 + p_3}{p_3+1} - \binom{p_1 + p_3}{p_3} & \text{if } p_2 = 1 \text{ and } p_1 > 1 \vspace{0.2cm} \\
        \binom{p_1 + p_2 + 1}{p_1} - \binom{p_1 + p_2 - 1}{p_2-1} + \sum_{i=p_2}^{p_1 + p_2 - 2} i \binom{i-1}{p_2 - 1} & \text{if } p_3 = 1 \text{ and } p_1,p_2 > 1 \vspace{0.2cm} \\
        U_{p_1,p_2,p_3} & \text{otherwise}
    \end{cases}
\end{equation}

Note here that the final value \(U_{p_1,p_2,p_3}\) corresponds to coefficients that are unknown and are currently investigated. In essence function \(G()\) takes as arguments a possible combination of numbers of \(\lambda^A, \lambda^o \text{ and } \mu \) for a given Markov model and outputs the coefficient of that term which in turn represents how many spanning trees exist in the graph with that specific combination. For instance consider the coefficients \((p_1,p_2,p_3)\) of some of the terms from the equations above:

\begin{multicols}{2}
    \begin{itemize}
        \item (\ref{eq:00_rate_1131}) \( \Rightarrow (\lambda^A)^2\): \(G(2,0,0) = \binom{2+0}{0} = 1\)
        \item (\ref{eq:00_rate_1131}) \( \Rightarrow \lambda^A \lambda^o\): \(G(1,1,0) = \binom{1+1-1}{1} = 1\)
        \item (\ref{eq:00_rate_1131}) \( \Rightarrow 2 \lambda^A \mu\): \(G(1,0,1) = \binom{1+1}{1} = 2\)
        \item (\ref{eq:00_rate_1131}) \( \Rightarrow \mu^2\): \(G(0,0,2) = 1\)
        \item (\ref{eq:00_rate_1141}) \( \Rightarrow 2(\lambda^A)^2 \lambda^o\): \(G(2,1,0) = \binom{2+1-1}{1} = 2\)
        \item (\ref{eq:00_rate_1141}) \( \Rightarrow 3(\lambda^A)^2 \mu\): \(G(2,0,1) = \binom{2+1}{1} = 3\)
        \item (\ref{eq:00_rate_1141}) \( \Rightarrow 3 \lambda^A \mu^2\): \(G(1,0,2) = \binom{1+2}{2} = 3\)
        \item (\ref{eq:00_rate_1151}) \( \Rightarrow 3 (\lambda^A)^3 \lambda^o\): \(G(3,1,0) = \binom{3+1-1}{1} = 3\)
        \item (\ref{eq:00_rate_1151}) \( \Rightarrow 3 (\lambda^A)^2 (\lambda^o)^2 \): \(G(2,2,0) = \binom{3}{2} = 3\)
        \item (\ref{eq:00_rate_1151}) \( \Rightarrow 6 (\lambda^A)^2 \mu ^ 2\): \(G(2,0,2) = \binom{2+2}{2} = 6\)
    \end{itemize}
\end{multicols}

\begin{itemize}
    \item (\ref{eq:00_rate_1151}) \( \Rightarrow 6 (\lambda^A)^2 \lambda^o \mu\): \(G(2,1,1) = \binom{2+1+1}{2} + 1\binom{2+1}{1+1} - \binom{2+1}{1} = 6 + 3 - 3 = 6\)
    \item \small{(e.g)} \( \Rightarrow (\lambda^A)^2 (\lambda^o)^2 \mu\): \(G(2,2,1) = \binom{2+2+1}{2} - \binom{2+2-1}{2-1} + \sum_{i=2}^{2+2-2} i\binom{i-1}{2-1} = 10 - 3 + (2 \times 1) = 9\)
\end{itemize}



\subsubsection{Unknown terms}

The terms that remain unknown are the terms where \(p_1, p_2, p_3 \geq 2\). For small models these terms are very few but as the model grows the number of these terms increase exponentially. Here are some of these values with the corresponding values of the \(G(p_1,p_2,p_3)\) function.

\begin{multicols}{2}
    \begin{itemize}
        \item \(G(2,2,2) = 18\) 
        \item \(G(3,2,2) = 60\)
        \item \(G(2,3,2) = 24\)
        \item \(G(2,2,3) = 30\)
        \item \(G(4,2,2) = 150\)
        \item \(G(3,3,2) = 100\)
        \item \(G(3,2,3) = 120\)
        \item \(G(2,4,2) = 30\)
        \item \(G(2,3,3) = 40\)
        \item \(G(2,2,4) = 45\)
    \end{itemize}
\end{multicols}

\subsubsection{Transforming the spanning tree problem into a permutation problem}

Given the nature of the spanning tree problem and the properties that the specific Markov chain model holds, this problem can be further reduced to another problem that may be slightly easier to solve. Consider the following Markov model and the spanning trees rooted at state \((0,0)\) that are associated with it. 

\begin{figure}[h]
    \centering
    \scalebox{0.7}{\input{MarkovChain/closed_form_state_probs/example_model_1131/main.tex}} \vspace{0.8cm} \\
    \scalebox{0.6}{\input{MarkovChain/closed_form_state_probs/spanning_trees_1131/main_0.tex}} \hspace{0.7cm}
    \scalebox{0.6}{\input{MarkovChain/closed_form_state_probs/spanning_trees_1131/main_1.tex}} \vspace{0.4cm} \\
    \scalebox{0.6}{\input{MarkovChain/closed_form_state_probs/spanning_trees_1131/main_2.tex}} \hspace{0.7cm}
    \scalebox{0.6}{\input{MarkovChain/closed_form_state_probs/spanning_trees_1131/main_3.tex}}\vspace{0.4cm} \\
    \scalebox{0.6}{\input{MarkovChain/closed_form_state_probs/spanning_trees_1131/main_4.tex}}
\end{figure}

Looking at these spanning trees from a different perspective it can be observed that all spanning trees of the specific model have some edges in common. 

\begin{multicols}{4}
    \begin{itemize}
        \item \((0,1) \rightarrow (0,0)\)
        \item \((1,1) \rightarrow (0,1)\)
        \item \((1,2) \rightarrow (1,1)\)
        \item \((1,3) \rightarrow (1,2)\)
    \end{itemize}
\end{multicols}

These edges are the ones on the bottom row of the model, on the \textit{threshold column} and on the \textit{arm} of the model. In general the set of edges that are present on all spanning trees can be denoted by:
\begin{align} \label{eq:common_edges_set}
    S &= S_1 \cup S_2 \cup S_3 \nonumber\\
    S_2 &= \{(M,v) \rightarrow (M,v-1) \; | \; T < v \leq N\} \nonumber \\
    S_1 &= \{(u,T) \rightarrow (u-1,T) \; | \; 0 < u \leq M\} \nonumber \\
    S_3 &= \{(0,v) \rightarrow (0,v-1) \; | \; 0 < v \leq T\} \nonumber \\
\end{align}

In addition, these edges that are common to every spanning tree have the same weight of \(mu\). In this specified model there are four of these edges, each with a weight of \(\mu\). Thus, since these edges exist on all spanning trees, the weight of every spanning tree must have include a term \(\mu^4\). Consider the expression of \(\tilde{\pi}_{(0,0)}\) associated with this Markov model:

\begin{equation}
    \tilde{\pi}_{(0,0)} = \mu^4[(\lambda^A)^2 + \lambda^A \lambda^o + 2\lambda^A \mu + \mu^2] 
\end{equation}

It can be seen that there is a \(\mu^4\) term that is a common factor of all spanning trees. These term can be more generally calculated as \(\mu^{M+N}\) and by excluding all edges associated with it can make the problem slightly easier to solve. 

\begin{figure}[h]
    \centering
    \scalebox{0.7}{
        \begin{tikzpicture}[-, node distance = 1cm, auto]
            \node[state] (u0v0) {(0,0)};
            \node[state, right=of u0v0] (u0v1) {(0,1)};
            \draw[->](u0v1) edge node {\(\mu \)} (u0v0);
            \node[state, below=of u0v1] (u1v1) {(1,1)};
            \draw[->](u1v1) edge node {\(\mu \)} (u0v1);
            \node[state, right=of u0v1] (u0v2) {(0,2)};
            \node[state, right=of u1v1] (u1v2) {(1,2)};
            \draw[->](u1v2) edge node {\(\mu \)} (u1v1);
            \node[state, right=of u0v2] (u0v3) {(0,3)};
            \node[state, right=of u1v2] (u1v3) {(1,3)};
            \draw[->](u1v3) edge node {\(\mu \)} (u1v2);
            % \draw[->](u0v2) edge node {\(\mu \)} (u0v1);
            % \draw[->](u0v3) edge node {\(\mu \)} (u0v2);
        \end{tikzpicture}
    }
\end{figure}

The specific problem has now been reduced to finding all possible combinations of two edges where one starts from \((0,2)\) and the other from \((0,3)\). The possible edges that can be utilised here may have a direction of either left, right, or down. Thus, the objective of the problem can be transformed into finding all possible permutations of an array of size \(2\) where elements can be \(L, R \text{ or } D\) and obey certain rules so that it correspond to a valid spanning tree. These rules are:

\begin{enumerate}
    \item Permutations ending with an \(R\) are not valid. \label{rule1}
    \item Permutations that have an \(R\) followed by an \(L\) are not valid. \label{rule2}
\end{enumerate}

If any of these two rules does not hold then the permutation is immediately valid. Rule \ref{rule1} points to the cases where the final state has an edge pointing to the right of it, which cannot occur since that state is the right-most state of the first row. Rule \ref{rule2} makes sure that there are no neighbour states that point to each other since that would create a cycle and would not generate a valid spanning tree.

For instance, consider the model above. Shown below, are all possible permutations of the array along with the excluded cases. The valid permutations (on the left) are shown in the same order with their corresponding spanning trees from the above figure and the invalid permutations (on the right) are followed by the rule that determines them invalid.

\begin{multicols}{2}
    \begin{itemize}
        \item \([D, D]\)
        \item \([L, D]\)
        \item \([D, L]\)
        \item \([R, D]\)
        \item \([L, L]\)
        \item \(\xcancel{[R, R]} \rightarrow \text{ Rule 1}\) 
        \item \(\xcancel{[L, R]} \rightarrow \text{ Rule 1}\)
        \item \(\xcancel{[R, L]} \rightarrow \text{ Rule 2}\)
        \item \(\xcancel{[D, R]} \rightarrow \text{ Rule 1}\)
    \end{itemize}
\end{multicols}